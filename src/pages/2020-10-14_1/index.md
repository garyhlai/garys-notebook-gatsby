---
path: "/20201014_1"
date: "2020-10-14"
title: "Transformer: Attention Walkthrough"
author: "Gary Lai"
tags: ["Machinelearning", "Python", "NLP"]
---

Per the name of [Attention is All You Need](https://arxiv.org/abs/1706.03762), attention is at the core of the Transformer. The general idea of attention has been explained many times before, but the specific implementation is still tricky. In the article, I will walk through the implementation of the attention mechanism to explain the details.
